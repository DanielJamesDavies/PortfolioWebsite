{
	"id": "turing-llm-1.0",
	"title": "Turing-LLM-1.0-254M",
	"siteUrl": "https://turingexplorer.com/inference",
	"codeUrl": "https://www.kaggle.com/models/danieljamesdavies/turing-llm-1.0-254m",
	"description": "A language model built for mechanistic interpretability research.\n\nTrained from initialization on a structured synthetic dataset. The text in the image are shortened outputs of Turing-LLM-1.0-254M.\n\n~254M Parameters, 12 Layers, SwiGLU Activation Function, Multi-Head Attention with 16 Heads\n\nGenerated a synthetic dataset using outputs from Phi-3 Mini and utilised data augmentation to expand the training dataset to 2B tokens.\n\nTrained 12 TopK sparse autoencoders to obtain a more monosemantic set of Turing-LLM-1.0 latents. Each with a hidden dimension size of 40,960.\n\nDeveloped novel mechanistic interpretability approaches which will be presented in a publication in the near future.",
	"tags": ["Python", "Pytorch", "Hugging Face Transformers", "NumPy"],
	"images": ["Turing-LLM-1.0-254M Examples.png", "Loss Curve over Training Steps.png", "Average Score per Internal Benchmark.png"]
}
