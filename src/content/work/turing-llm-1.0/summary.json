{
	"id": "turing-llm-1.0",
	"title": "Turing-LLM-1.0-254M",
	"description": "Trained an LLM from initialization on a synthetic dataset. The text in the image are shortened outputs of Turing-LLM-1.0-254M.\n\n~254M Parameters, 12 Layers, SwiGLU Activation Function, Multi-Head Attention with 16 Heads\n\nCreated a synthetic dataset using outputs from Phi-3 Mini and used data augmentation to expand the dataset to 2B tokens.\n\nTrained 12 sparse autoencoders (40k hidden dim) for interpretability research on reading the internals of Turing-LLM-1.0.\n\nDeveloped novel mechanistic interpretability approaches which will be presented in a publication in the near future.",
	"tags": ["Python", "Pytorch", "Hugging Face Transformers", "NumPy"],
	"images": ["Turing-LLM-1.0-254M Examples.png", "Loss Curve over Training Steps.png", "Average Score per Internal Benchmark.png"]
}
